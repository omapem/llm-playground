# Training configuration for GPT-2 Medium
# Model: 355M parameters, trained on C4 dataset
# Requires GPU with 40GB+ VRAM (A100)

model_config:
  vocab_size: 50257
  hidden_size: 1024
  num_layers: 24
  num_heads: 16
  intermediate_size: 4096
  max_position_embeddings: 1024
  activation: gelu
  dropout_rate: 0.1
  norm_first: true
  position_encoding_type: learned
  tie_embeddings: true

# Training hyperparameters
batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 5.0e-4
warmup_steps: 3000
max_steps: 150000
weight_decay: 0.01
max_grad_norm: 1.0
max_seq_length: 1024

# Optimizer and scheduler
optimizer_type: adamw
scheduler_type: cosine
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8

# Mixed precision and optimization
mixed_precision: bf16
gradient_checkpointing: true  # Enable for memory efficiency

# Logging and checkpointing
logging_steps: 100
save_steps: 2000
eval_steps: 10000
checkpoint_dir: ./checkpoints/gpt2_medium
max_checkpoints_to_keep: 3

# Distributed training (single GPU with large batch via accumulation)
num_devices: 1
use_ddp: false

# Weights & Biases integration
wandb_project: llm-playground
wandb_entity: null
wandb_run_name: gpt2-medium-c4

# Resume training
resume_from_checkpoint: null
