# GPT-2 Medium on WikiText-103
# Use case: Mid-scale pre-training for educational purposes
# Expected time: ~8-10 hours on A100 for 50K steps

model_config:
  vocab_size: 50257              # GPT-2 vocabulary size
  hidden_size: 1024              # Model dimension (medium size)
  num_layers: 24                 # Transformer layers
  num_heads: 16                  # Attention heads
  intermediate_size: 4096        # FFN dimension (4 * hidden_size)
  max_position_embeddings: 1024  # Maximum sequence length
  activation: gelu               # Activation function
  dropout_rate: 0.1              # Dropout probability
  position_encoding_type: learned
  layer_norm_type: layer_norm

# Training hyperparameters
batch_size: 4                    # Batch size per device (smaller for larger model)
gradient_accumulation_steps: 8   # Gradient accumulation (effective batch = 32)
learning_rate: 0.0003            # Peak learning rate (lower for larger model)
warmup_steps: 2000               # Warmup steps
max_steps: 50000                 # Total training steps
weight_decay: 0.01               # Weight decay for regularization
max_grad_norm: 1.0               # Gradient clipping threshold

# Optimizer settings
optimizer_type: adamw
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-08

# Learning rate scheduler
scheduler_type: cosine           # Cosine annealing with warmup

# Mixed precision training
mixed_precision: bf16            # Essential for medium-size models

# Logging and checkpointing
logging_steps: 100
save_steps: 1000
eval_steps: 500
checkpoint_dir: ./checkpoints/gpt2_medium_wikitext
max_checkpoints_to_keep: 5

# Distributed training
num_devices: 1
use_ddp: false

# Weights & Biases tracking
wandb_project: llm-playground
wandb_entity: null
wandb_run_name: gpt2-medium-wikitext-v1

# Resume training
resume_from_checkpoint: false
