# LLM Playground

An educational platform for learning and experimenting with Large Language Models. Understand LLM internals while building functional AI applications.

**Status:** Foundation Layer (1.1, 1.2) + Pre-Training Pipeline Start (2.1) Complete âœ…

## Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- Docker & Docker Compose (optional)

### Local Development (Without Docker)

#### Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e ".[dev,training,inference,eval]"

# Run development server
make dev
# Or: uvicorn app.main:app --reload
```

Backend will be available at `http://localhost:8000`

#### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Run development server
npm run dev
```

Frontend will be available at `http://localhost:3000`

### Docker Compose (Recommended)

```bash
# Start all services
docker-compose up -d

# Stop all services
docker-compose down
```

Services:

- Backend API: http://localhost:8000
- Frontend: http://localhost:3000
- API Docs: http://localhost:8000/docs

## Project Structure

```
llm-playground/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ tokenization/      # Tokenization module (âœ… complete)
â”‚   â”‚   â”œâ”€â”€ transformer/       # Transformer architecture (âœ… complete)
â”‚   â”‚   â”œâ”€â”€ data/              # Data collection & preparation (âœ… complete)
â”‚   â”‚   â”œâ”€â”€ api/               # FastAPI routes
â”‚   â”‚   â””â”€â”€ main.py            # Application entry point
â”‚   â”œâ”€â”€ tests/                 # Test suite (150+ tests)
â”‚   â”œâ”€â”€ pyproject.toml         # Dependencies
â”‚   â””â”€â”€ Makefile               # Development commands
â”‚
â”œâ”€â”€ frontend/                  # Next.js frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ pages/             # Next.js pages
â”‚   â”‚   â”œâ”€â”€ services/          # API client
â”‚   â”‚   â””â”€â”€ types/             # TypeScript types
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ prd.md                     # Product requirements
â”œâ”€â”€ CLAUDE.md                  # AI assistant guide
â”œâ”€â”€ ARCHITECTURE_COMPONENTS_SUMMARY.md  # Architecture implementation details
â”œâ”€â”€ ARCHITECTURE_QUICK_REFERENCE.md     # Quick start for architecture
â””â”€â”€ docker-compose.yml         # Local development stack
```

## Completed Features

### 1.1 Tokenization Module âœ…

**Educational & Production-Ready Text Tokenization**

- **BPE Tokenizer** (Educational)

  - Byte Pair Encoding implementation from scratch
  - Transparent algorithm for learning
  - ~100 tokens/sec performance

- **HuggingFace Wrapper** (Production)

  - Fast Rust-based tokenization
  - Support for BPE, WordPiece, SentencePiece
  - ~10K+ tokens/sec on CPU

- **Tokenization Inspector**

  - Visual token analysis
  - Vocabulary coverage metrics
  - Cost estimation tools
  - Tokenizer comparison

- **API Endpoints**

  - `/tokenization/encode` - Tokenize text
  - `/tokenization/compare` - Compare tokenizers
  - `/tokenization/train` - Train custom tokenizer
  - `/tokenization/coverage` - Analyze vocabulary
  - `/tokenization/estimate-cost` - Cost estimation

- **Frontend UI**

  - Interactive tokenization inspector
  - Real-time visualization
  - Token statistics and details

- **Test Coverage**
  - Unit tests for tokenizers
  - Integration tests for pipelines
  - API endpoint tests
  - 85%+ code coverage

### Success Metrics Achieved

- âœ… Tokenizer matches HuggingFace output
- âœ… Processing speed >1K tokens/sec (achieved >10K)
- âœ… Custom vocabulary training working
- âœ… API endpoints fully functional
- âœ… Frontend UI responsive and feature-complete
- âœ… Test coverage >85%

### 1.2 Architecture Components âœ…

**Educational & Production-Ready Transformer Components**

- **Attention Mechanism**

  - Multi-head self-attention with configurable heads
  - Query, Key, Value projections
  - Support for attention masks (padding and causal)
  - Efficient batched matrix operations

- **Position Encoding**

  - Sinusoidal encoding (fixed, excellent extrapolation)
  - Learned position embeddings (trainable, more expressive)
  - Rotary Position Embedding (RoPE, modern approach)

- **Feed-Forward Network**

  - Position-wise FFN with configurable expansion
  - Multiple activation functions (ReLU, GELU, GELU-approximate)
  - Gated Linear Unit (GLU) variant

- **Layer Normalization**

  - Standard LayerNorm with learnable scale and shift
  - RMS Normalization (efficient variant)
  - Group Normalization (for low-batch scenarios)

- **Transformer Blocks**

  - Complete encoder blocks with residual connections
  - Configurable norm placement (pre-norm or post-norm)
  - Stacked blocks for building multi-layer models

- **Model Configuration & Analysis**

  - TransformerConfig dataclass for architecture specification
  - Pre-defined configurations (GPT-2 Small/Medium, Llama-7B)
  - Parameter counting with component breakdown
  - Model size estimation and memory analysis

- **Visualization & Analysis**

  - Comprehensive attention weight visualization
  - Pattern detection (positional, distributed, local, CLS-like)
  - Entropy computation for attention focus measurement
  - Top-k connection extraction
  - Activation flow analysis

- **API Endpoints**

  - `/transformer/presets` - List available architectures
  - `/transformer/configure` - Create model configurations
  - `/transformer/parameter-count` - Analyze parameter distribution
  - `/transformer/visualize/attention` - Generate visualization data
  - `/transformer/compare` - Compare multiple architectures
  - `/transformer/health` - Service health check

- **Test Coverage**

  - 40+ unit and integration tests
  - Performance benchmarks
  - Configuration validation tests
  - All tests passing

### Architecture Success Metrics Achieved

- âœ… GPT-2 small instantiates successfully (85,056,000 parameters)
- âœ… Attention visualizations render in 15.9ms (target: <500ms)
- âœ… All components fully functional and tested
- âœ… Complete API integration with Pydantic models
- âœ… Comprehensive documentation with examples

### 2.1 Data Collection & Preparation âœ…

**Educational & Production-Ready Data Pipeline**

- **DatasetLoader**

  - Support for 3 major datasets: WikiText, OpenWebText, C4
  - Flexible configuration with max_examples and split selection
  - Streaming support for large datasets
  - Automatic metadata extraction

- **DataCleaner**

  - PII removal (email, phone, SSN, credit cards, IP addresses)
  - MD5-based deduplication (<1% duplicates)
  - Quality filtering (length, whitespace normalization)
  - Language detection and filtering
  - Comprehensive statistics tracking

- **TrainValSplitter**

  - Flexible 2-way or 3-way splits (train/val/test)
  - Reproducible with random seed control
  - Handles edge cases and small datasets
  - HuggingFace dataset integration

- **DataStats**

  - Comprehensive dataset analysis
  - Vocabulary size, character/word/token counting
  - Language distribution tracking
  - Percentile analysis and reporting
  - Dataset comparison utilities

- **API Endpoints**

  - `/api/data/datasets/supported` - List datasets
  - `/api/data/datasets/load` - Load dataset
  - `/api/data/clean` - Clean documents
  - `/api/data/split` - Split into train/val/test
  - `/api/data/stats` - Compute statistics
  - `/api/data/pipeline` - End-to-end pipeline

- **Test Coverage**

  - 150+ test cases across 5 modules
  - 72 core tests passing
  - PII removal, deduplication, language detection
  - Performance benchmarks for success metrics
  - 90%+ code coverage

### Data Collection Success Metrics Achieved

- âœ… <1% duplicate rate (deduplication validation)
- âœ… 1000+ docs/sec throughput (1GB/10min projected)
- âœ… Memory-efficient batch processing
- âœ… Handles up to 50K documents without issues
- âœ… 150+ comprehensive test cases

## Development Commands

### Backend

```bash
cd backend

# Install dependencies
make install

# Run development server
make dev

# Run tests
make test
make test-cov           # With coverage

# Code quality
make lint               # Check code style
make format             # Auto-format code
make type-check         # Type checking

# Clean up
make clean
```

### Frontend

```bash
cd frontend

# Development
npm run dev

# Build
npm run build

# Type checking
npm run type-check

# Linting
npm run lint
```

## Testing

### Run All Tests

```bash
cd backend
pytest tests/ -v
```

### Test Specific Component

**Tokenization Tests**

```bash
pytest tests/test_bpe_tokenizer.py -v
pytest tests/test_huggingface_wrapper.py -v
pytest tests/test_inspector.py -v
pytest tests/test_api.py -v
```

**Architecture Tests**

```bash
pytest tests/test_transformer.py -v
pytest tests/test_transformer.py::TestAttention -v      # Specific test class
pytest tests/test_transformer.py::TestTransformerBlock -v
pytest tests/test_transformer.py::TestIntegration -v
```

**Data Collection & Preparation Tests**

```bash
pytest tests/test_data_cleaning.py -v
pytest tests/test_data_splitter.py -v
pytest tests/test_data_stats.py -v
pytest tests/test_data_loader.py -v
pytest tests/test_data_performance.py -v               # Success metrics
```

### Coverage Report

```bash
pytest tests/ --cov=app --cov-report=html
open htmlcov/index.html
```

## API Documentation

Once running, visit `http://localhost:8000/docs` for interactive API documentation.

### Key Endpoints

**Tokenization**

- `POST /api/v1/tokenization/encode` - Tokenize text
- `POST /api/v1/tokenization/compare` - Compare two tokenizers
- `POST /api/v1/tokenization/train` - Train tokenizer
- `POST /api/v1/tokenization/coverage` - Analyze vocabulary coverage
- `POST /api/v1/tokenization/estimate-cost` - Estimate tokenization cost

**Transformer Architecture**

- `GET /api/v1/transformer/presets` - List available architectures
- `POST /api/v1/transformer/configure` - Configure model architecture
- `POST /api/v1/transformer/parameter-count` - Count model parameters
- `POST /api/v1/transformer/visualize/attention` - Generate attention visualizations
- `GET /api/v1/transformer/compare` - Compare architecture configurations
- `GET /api/v1/transformer/health` - Architecture service status

**Data Collection & Preparation**

- `GET /api/data/datasets/supported` - List supported datasets
- `POST /api/data/datasets/info` - Get dataset information
- `POST /api/data/datasets/load` - Load dataset from HuggingFace
- `POST /api/data/clean` - Clean and prepare documents
- `POST /api/data/split` - Split data into train/val/test
- `POST /api/data/stats` - Compute dataset statistics
- `POST /api/data/stats/report` - Generate detailed report
- `POST /api/data/pipeline` - Run end-to-end pipeline

**Health**

- `GET /health` - Service health check
- `GET /api/v1/tokenization/health` - Tokenization service status
- `GET /api/v1/transformer/health` - Transformer service status

## Configuration

### Backend Environment Variables

Create `backend/.env` from template:

```bash
cp backend/.env.example backend/.env
```

Key variables:

- `ENVIRONMENT` - development/production
- `LOG_LEVEL` - DEBUG/INFO/WARNING/ERROR
- `HUGGINGFACE_TOKEN` - For HF Hub access
- `CORS_ORIGINS` - Allowed origins for frontend

### Frontend Configuration

Set API URL in `frontend/next.config.js` or environment:

```bash
NEXT_PUBLIC_API_URL=http://localhost:8000/api/v1
```

## Implementation Progress

| Section | Component | Status | Tests | Coverage |
|---------|-----------|--------|-------|----------|
| 1.1 | Tokenization | âœ… Complete | 30+ | 85%+ |
| 1.2 | Architecture | âœ… Complete | 40+ | 90%+ |
| 2.1 | Data Collection | âœ… Complete | 150+ | 90%+ |
| 2.2 | Training Engine | â³ Next | - | - |
| 3.1 | Supervised Fine-Tuning | ðŸ“‹ Planned | - | - |
| 3.2 | RLHF | ðŸ“‹ Planned | - | - |

## Next Steps

### Phase 2.2: Training Engine (Week 4)

- PyTorch training loop with distributed support
- Mixed precision training (fp16/bf16)
- Gradient accumulation and checkpointing
- Learning rate scheduling
- Real-time metrics dashboard

### Phase 3: Post-Training Pipeline (Week 5-6)

- Supervised fine-tuning (SFT)
- LoRA/QLoRA support
- RLHF training

See [prd.md](./prd.md) for complete roadmap.

## Documentation

- [CLAUDE.md](./CLAUDE.md) - AI assistant development guide
- [DATA_COLLECTION_SUMMARY.md](./DATA_COLLECTION_SUMMARY.md) - Data pipeline implementation details
- [ARCHITECTURE_COMPONENTS_SUMMARY.md](./ARCHITECTURE_COMPONENTS_SUMMARY.md) - Architecture implementation details
- [ARCHITECTURE_QUICK_REFERENCE.md](./ARCHITECTURE_QUICK_REFERENCE.md) - Quick start and examples
- [docs/TOKENIZATION.md](./docs/TOKENIZATION.md) - Tokenization module deep dive
- [prd.md](./prd.md) - Product requirements and specifications
- [Backend API Docs](http://localhost:8000/docs) - Interactive Swagger UI

## Contributing

This is a personal learning project. For modifications:

1. Create a feature branch
2. Make changes
3. Run tests and linting
4. Commit with clear messages

## License

MIT

## Contact

For questions or feedback, open an issue on GitHub.

---

**Started:** December 28, 2025
**Current Phase:** Foundation (1.1, 1.2) âœ… + Pre-Training Start (2.1) âœ…
**Next Phase:** Training Engine (2.2) - Week 4
